{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to extend the appearance of Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extention of the rows and columns depiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 600)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Columns Based on _ and Lowercase Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_column_names(df,rename_dict={},do_inplace=True):\n",
    "    if not rename_dict:\n",
    "        return df.rename(columns={col: col.lower().replace(' ','_') \n",
    "                    for col in df.columns.values.tolist()}, \n",
    "                  inplace=do_inplace)\n",
    "    else:\n",
    "        return df.rename(columns=rename_dict,inplace=do_inplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating the Title and Review Text Columns (Based on Either Non-Null Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df[df.title.notnull() | df.review_text.notnull()]\n",
    "df2.review_text.astype(str)\n",
    "df2.title.astype(str)\n",
    "df2['new_text'] = df2[['title', 'review_text']].apply(lambda x: ' '.join(str(y) for y in x if str(y) !='nan'), axis=1)\n",
    "df2.drop('title', axis = 1, inplace = True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "sns.set()\n",
    "_ = sns.heatmap(df.isnull(),yticklabels=False, cbar = False, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['pickup_city'] = df['pickup_city'].apply(lambda x: x.rstrip(' ') if x.endswith(' ') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[~df['estimated_ship_date'].str.contains('N')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[~df['estimated_ship_date'].str.startswith('20') | df['estimated_ship_date'].str.startswith('202')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df[df.type.isin(['Car','SUV','Pickup','Motorcycle','Van'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['dropoff_zip'] = df['dropoff_zip'].str.extract('(\\d{5})', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[pd.notnull(df['pickup_zip'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df['pickup_city'].str.contains('0'or'1'or'2' or '3' or '4' or '5' or '6' or '7' or '8' or '9')].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipped_list = list(zip(df['pickup_zip'], df['dropoff_zip']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df1.apply(lambda x:x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['make'].replace(r'Old.*', 'Oldsmobile', inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals_to_replace = {'chevolet':'chevrolet', 'chevypickup':'chevrolet', 'chev':'chevrolet',\n",
    "                  'chev.':'chevrolet', 'chevy`':'chevrolet', 'chevevolet':'chevrolet','cheverelot':'chevrolet'}\n",
    "df = df.replace({'make': vals_to_replace})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(df[df['estimated_ship_date'].str.contains('N')].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.year = pd.to_numeric(df.year, errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.index = pd.RangeIndex(len(df.index))\n",
    "\n",
    "df.index = range(len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby(\"make\").filter(lambda x: len(x) >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.make.replace({'chevroletrolet': 'chevrolet'}, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df1.merge(df, on='install_at_site_use_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'location': 'install_at_site_use_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DG=df.groupby(['A', 'C'])   \n",
    "pd.concat([DG.get_group(item) for item, value in DG.groups.items() if len(value)==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First character is alpha or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['host_id'] = df2['host_id'].apply(lambda x: x if x[0].isalpha() else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing multiple csv files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path =r'C:\\Users\\mkadiogl\\Desktop\\Licensing\\Cleaned_1M_databases' # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "list_ = []\n",
    "\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "\n",
    "frame = pd.concat(list_, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Multiple Excel Files at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_id = pd.DataFrame()\n",
    "for f in ['install_at_site_{}.xls'.format(i) for i in range(1,75)]:\n",
    "    data = pd.read_excel(f, sheet_name=\"Sheet 1\")\n",
    "    df_h_id = df_h_id.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Two Columns with ',' Seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['dropoff_comp'] = df['dropoff_city'].str.cat(df['dropoff_state_code'], sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda If Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['rating_class'] = df2['rating'].apply(lambda x: 'bad' if x < 3 else('good' if x > 3 else 'neutral'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list2 = list()\n",
    "for x in df2.city_dist:\n",
    "    try:  \n",
    "        pick = x.split('-')[0]\n",
    "        drop = x.split('-')[1]    \n",
    "    #print (pick , drop)\n",
    "        url = \"https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins={}&destinations={}&key={}\".format(pick,drop,api)\n",
    "        #print (url)\n",
    "        res = urllib.request.urlopen(url).read()\n",
    "        data = json.loads(res.decode())\n",
    "        #print (data)\n",
    "        #print(data[\"rows\"][0][\"elements\"][0][\"distance\"][\"value\"]/1000)\n",
    "        ##df['distance'][index] = int(data[\"rows\"][0][\"elements\"][0][\"distance\"][\"text\"].split()[0].replace(',',''))\n",
    "        ##df['distance'][index] = int(data[\"rows\"][0][\"elements\"][0][\"distance\"][\"value\"])\n",
    "        list2.append((data[\"rows\"][0][\"elements\"][0][\"distance\"][\"text\"]))\n",
    "    except:\n",
    "        #df['distance'][index] = (np.nan)\n",
    "        list2.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dateutil - pip install python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import *\n",
    "from dateutil.easter import *\n",
    "from dateutil.rrule import *\n",
    "from dateutil.parser import *\n",
    "from datetime import *\n",
    "now = parse(\"Sat Oct 11 17:13:46 UTC 2003\")\n",
    "today = now.date()\n",
    "year = rrule(YEARLY,dtstart=now,bymonth=8,bymonthday=13,byweekday=FR)[0].year\n",
    "rdelta = relativedelta(easter(year), today)\n",
    "print(\"Today is: %s\" % today)\n",
    "print(\"Year with next Aug 13th on a Friday is: %s\" % year)\n",
    "print(\"How far is the Easter of that year: %s\" % rdelta)\n",
    "print(\"And the Easter of that year is: %s\" % (today+rdelta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DF With List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(\n",
    "    {'year': year,\n",
    "     'model': model,\n",
    "     'make': make,\n",
    "     'type': type1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime and Calender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import calendar\n",
    "\n",
    "df.event_time=pd.to_datetime(df.estimated_ship_date, errors='coerce')\n",
    "df['event_year'] = df.event_time.map(lambda x: x.year)\n",
    "df['event_month'] = df.event_time.map(lambda x: x.month)\n",
    "df['event_month_name'] = df['event_month'].apply(lambda x: calendar.month_name[int(x)] if not np.isnan(x) else np.nan)\n",
    "df['event_day'] = df.event_time.map(lambda x: x.day)\n",
    "df['event_day_of_week'] = df.event_time.dt.weekday_name\n",
    "df['event_day_of_week_number'] = df.event_time.dt.weekday\n",
    "df['event_hour'] = df.event_time.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Any Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install inflect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotly.tools.set_credentials_file(username='', api_key='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the notebook to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jupyter nbconvert --to html mynotebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "img = 'TF_IDF_3_class.jpg'\n",
    "Image(filename=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Programming (TPOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpot = TPOTRegressor(max_time_minutes = 60,\n",
    "                     generations=5, \n",
    "                     population_size=20, \n",
    "                     verbosity=2,\n",
    "                    n_jobs = -1,\n",
    "                    cv = 5)\n",
    "tpot.fit(X_scaled, y_train)\n",
    "print(tpot.score(X_test_scaled, y_test))\n",
    "#tpot.export('tpot_boston_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=None)\n",
    "\n",
    "exported_pipeline = GradientBoostingRegressor(alpha=0.85, learning_rate=0.1, loss=\"ls\",\n",
    "                                              max_features=0.9, min_samples_leaf=5,\n",
    "                                              min_samples_split=6)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_classes)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=None)\n",
    "\n",
    "\n",
    "exported_pipeline = KNeighborsClassifier(n_neighbors=6, weights=\"distance\")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_classes)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Normalize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto ML and Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from auto_ml import Predictor\n",
    "#from catboost import CatBoostRegressor\n",
    "import pickle\n",
    "\n",
    "column_descriptions = {\n",
    "  'price_total': 'output', 'pickup_city': 'categorical', 'dropoff_city': 'categorical', \n",
    "    'pickup_state_code': 'categorical', 'dropoff_state_code': 'categorical', 'make': 'categorical', \n",
    "    'model': 'categorical'} #  no need for 'type': 'categorical'\n",
    "\n",
    "\n",
    "types = df[\"type\"].unique()\n",
    "\n",
    "#types=[\"Car\"]\n",
    "\n",
    "for t in types: # this would be parallelised later\n",
    "    \n",
    "    temp_df = df[df[\"type\"] == t].drop([\"type\"], axis=1)\n",
    "\n",
    "    #df_train, df_test = train_test_split(temp_df, test_size=0.2)\n",
    "\n",
    "    ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)\n",
    "\n",
    "    print ('training started for vehicle type >> ', t)\n",
    "    \n",
    "    ml_predictor.train(temp_df, model_names='GradientBoostingRegressor', cv=5)\n",
    "    \n",
    "    score = r2_score(temp_df.price_total, ml_predictor.predict(temp_df))\n",
    "\n",
    "    print ('score for {} is {}'.format(t, round(score,4)))\n",
    "    \n",
    "    filename = './models_by_types1/GB_model_{}_{}.ml'.format(t, score)\n",
    "    \n",
    "    pickle.dump(ml_predictor, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name=\"./models_by_types1/GB_model_Car_0.9009313891926389.ml\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "loaded_model = pickle.load(open(model_name, 'rb'))\n",
    "\n",
    "temp_df=df[df[\"type\"] == \"Car\"].drop([\"type\"], axis=1)\n",
    "\n",
    "loaded_model.predict(temp_df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# API_Auto_ML_Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "import pickle\n",
    "import flask\n",
    "\n",
    "#model_name=\"./models_by_types1/GB_model_Car_0.9009313891926389.ml\"\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "#getting our trained model from a file we created earlier\n",
    "#model = pickle.load(open(model_name,\"rb\"), encoding ='utf-8')\n",
    "model_name=\"./full_model/GB_model_0.9024795449595084.ml\"\n",
    "\n",
    "loaded_model = pickle.load(open(model_name, 'rb'))\n",
    "\n",
    "#loaded_model.predict(df1[10:20])\n",
    "\n",
    "@app.route('/predict', methods=['POST','GET'])\n",
    "def predict():\n",
    "    try:\n",
    "        feature_array = flask.request.get_json(silent=True)\n",
    "        \n",
    "        print (feature_array)   \n",
    "    except Exception as e:\n",
    "        \n",
    "        return jsonify({'success':False, \"error\": str(e)})\n",
    "    \n",
    "    df1=pd.DataFrame([feature_array], columns=feature_array.keys())    \n",
    "    \n",
    "    # convert this json to dataframe\n",
    "    prediction = loaded_model.predict(df1)\n",
    "    \n",
    "    return flask.jsonify({'success':True, \"predictions\": prediction})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_simple('192.168.1.62', 8081, app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "payload=json.loads(df.iloc[9].to_json())\n",
    "api_call=requests.post(\"http://192.168.1.62:8081/predict\", json=payload)\n",
    "json.loads(api_call.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api_call.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating a New Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!mkdir filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Distance Matrix API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "api = 'AIzaSyAFfa7OW7CLVZ7ZSP36avc1f8BQudNDlOM'\n",
    "pick = \n",
    "drop = \n",
    "url = \"https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins={}&destinations={}&key={}\".format(pick,drop,api)\n",
    "    #print (url)\n",
    "res = urllib.request.urlopen(url).read()\n",
    "data = json.loads(res.decode())\n",
    "    #print (data)\n",
    "    #print(data[\"rows\"][0][\"elements\"][0][\"distance\"][\"text\"])\n",
    "print(data[\"rows\"][0][\"elements\"][0][\"distance\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CATBOOST API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_predictor = CatBoostRegressor(iterations=1000, depth=10, learning_rate=0.05, loss_function='RMSE', eval_metric='R2')\n",
    "\n",
    "#print (\"training started for vehicle type >> \", t)\n",
    "    \n",
    "ml_predictor.fit(X_train, y_train, cat_features=categorical_features_indices, eval_set=(X_test, y_test),plot=False) \n",
    "\n",
    "score = r2_score(y_test, ml_predictor.predict(X_test))\n",
    "    \n",
    "print (\"score is {}\".format(round(score,4)))\n",
    "filename = './model_catboost/CB_model_{}.ml'.format(score)\n",
    "ml_predictor.save_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "import pickle\n",
    "import flask\n",
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import dill\n",
    "from catboost import CatBoostRegressor\n",
    "import datetime as dt\n",
    "import calendar\n",
    "\n",
    "#model_name=\"./models_by_types1/GB_model_Car_0.9009313891926389.ml\"\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "#getting our trained model from a file we created earlier\n",
    "#model = pickle.load(open(model_name,\"rb\"), encoding ='utf-8')\n",
    "ml_predictor = CatBoostRegressor(iterations=1000, depth=10, learning_rate=0.05, loss_function='RMSE', eval_metric='R2')\n",
    "\n",
    "model_name=\"./model_catboost/CB_model_0.8825854610673416.coreml\"\n",
    "\n",
    "ml_predictor.load_model(model_name)\n",
    "\n",
    "#loaded_model = pickle.load(open(model_name, 'rb'))\n",
    "\n",
    "def distance (pickup_city, pickup_state, dropoff_city, dropoff_state):\n",
    "    \n",
    "    pickup_city = pickup_city.replace(' ', '+') if ' ' in pickup_city else pickup_city \n",
    "       \n",
    "    pick = pickup_city + ',' + pickup_state\n",
    "    \n",
    "    dropoff_city = dropoff_city.replace(' ', '+') if ' ' in dropoff_city else dropoff_city\n",
    "    \n",
    "    drop = dropoff_city + ',' + dropoff_state \n",
    "    \n",
    "    api = 'AIzaSyAFfa7OW7CLVZ7ZSP36avc1f8BQudNDlOM'\n",
    "\n",
    "    try:\n",
    "        url = \"https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins={}&destinations={}&key={}\".format(pick, drop, api)\n",
    "        \n",
    "        res = urllib.request.urlopen(url).read()\n",
    "        \n",
    "        data = json.loads(res.decode())\n",
    "    \n",
    "        distance = round((data[\"rows\"][0][\"elements\"][0][\"distance\"][\"value\"]/1609),2)\n",
    "    except:\n",
    "        distance =0\n",
    "    \n",
    "    return distance\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST','GET'])\n",
    "\n",
    "def predict():\n",
    "    try:\n",
    "        feature_array = flask.request.get_json()\n",
    "        \n",
    "        print (feature_array)   \n",
    "    except Exception as e:\n",
    "        \n",
    "        return jsonify({'success':False, \"error\": str(e)})\n",
    "  \n",
    "    \n",
    "    feature_array['make']=feature_array['make'].lower()\n",
    "    \n",
    "    feature_array['model']= feature_array['make'].lower()\n",
    "\n",
    "    feature_array[\"distance\"] = distance (feature_array['pickup_city'],\n",
    "                                          feature_array['pickup_state_code'],\n",
    "                                          feature_array['dropoff_city'],\n",
    "                                          feature_array['dropoff_state_code'])\n",
    "    \n",
    "    if feature_array[\"distance\"] == 0:\n",
    "        \n",
    "        return flask.jsonify({'success':False, \"error\": \"something wrong with Google matrix distance API\"})\n",
    "    \n",
    "    df=pd.DataFrame([feature_array], columns=feature_array.keys()) \n",
    "    \n",
    "    df.estimated_ship_date=pd.to_datetime(df.estimated_ship_date, errors='coerce')\n",
    "    df['event_year'] = df.estimated_ship_date.map(lambda x: x.year)\n",
    "    df['event_month'] = df.estimated_ship_date.map(lambda x: x.month)\n",
    "    df['event_month_name'] = df['event_month'].apply(lambda x: calendar.month_name[int(x)] if not np.isnan(x) else np.nan)\n",
    "    df['event_day'] = df.estimated_ship_date.map(lambda x: x.day)\n",
    "    df['event_day_of_week_number'] = df.estimated_ship_date.dt.weekday\n",
    "    \n",
    "    #df.estimated_ship_date=pd.to_datetime(df.estimated_ship_date, errors=\"coerce\")\n",
    "    df=df[['pickup_state_code', 'dropoff_state_code', 'vehicle_runs', 'make', 'model', 'type', 'year', 'diesel_price', \n",
    "           'event_year', 'event_month_name', 'event_day', \n",
    "           'event_day_of_week_number','distance','pickup_city', 'dropoff_city', 'ship_via_id']]\n",
    "    \n",
    "    print (df)\n",
    "    df.to_csv('df_xx.csv', index=False)\n",
    "    \n",
    "    # convert this json to dataframe,\n",
    "    prediction = ml_predictor.predict(df)\n",
    "\n",
    "    return flask.jsonify({'success':True, \"predictions\": prediction[0]})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     run_simple('192.168.1.62', 8081, app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# API_H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "import pickle\n",
    "import flask\n",
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import dill\n",
    "import h2o\n",
    "\n",
    "\n",
    "#model_name=\"./models_by_types1/GB_model_Car_0.9009313891926389.ml\"\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "#getting our trained model from a file we created earlier\n",
    "#model = pickle.load(open(model_name,\"rb\"), encoding ='utf-8')\n",
    "\n",
    "model_path = (r\"C:\\Users\\Mike\\Desktop\\Truck\\full_model_H2O\\modell_h2o\\dnn_default\")\n",
    "\n",
    "saved_model = h2o.load_model(model_path)\n",
    "\n",
    "\n",
    "\n",
    "#loaded_model.predict(df1[10:20])\n",
    "\n",
    "def distance (pickup_city, pickup_state_code, dropoff_city, dropoff_state_code):\n",
    "    \n",
    "    pickup_city = pickup_city.replace(' ', '+') if ' ' in pickup_city else pickup_city \n",
    "       \n",
    "    pick = pickup_city + ',' + pickup_state_code\n",
    "    \n",
    "    dropoff_city = dropoff_city.replace(' ', '+') if ' ' in dropoff_city else dropoff_city\n",
    "    \n",
    "    drop = dropoff_city + ',' + dropoff_state_code\n",
    "    \n",
    "    api = 'AIzaSyAFfa7OW7CLVZ7ZSP36avc1f8BQudNDlOM'\n",
    "\n",
    "    try:\n",
    "        url = \"https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins={}&destinations={}&key={}\".format(pick, drop, api)\n",
    "\n",
    "        res = urllib.request.urlopen(url).read()\n",
    "    \n",
    "        data = json.loads(res.decode())\n",
    "    \n",
    "        distance = round((data[\"rows\"][0][\"elements\"][0][\"distance\"][\"value\"]/1609),2)\n",
    "        print (distance)\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        distance = 0\n",
    "        \n",
    "    return distance\n",
    "\n",
    "@app.route('/predict', methods=['POST','GET'])\n",
    "\n",
    "def predict():\n",
    "    try:\n",
    "        feature_array = flask.request.get_json()\n",
    "        \n",
    "        print (feature_array)   \n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        return jsonify({'success':False, \"error\": str(e)})\n",
    "\n",
    "    feature_array[\"distance\"] = distance (feature_array['pickup_city'],\n",
    "                                          feature_array['pickup_state_code'],\n",
    "                                          feature_array['dropoff_city'],\n",
    "                                          feature_array['dropoff_state_code'])\n",
    "    if feature_array[\"distance\"] == 0:\n",
    "        \n",
    "        return flask.jsonify({'success':False, \"error\": \"something wrong with Google matrix distance API\"})\n",
    "\n",
    "    df1=pd.DataFrame([feature_array], columns=feature_array.keys())    \n",
    "    \n",
    "    hf = h2o.H2OFrame(df1)\n",
    "    \n",
    "    # convert this json to dataframe,\n",
    "    \n",
    "    prediction = saved_model.predict(hf)\n",
    "    \n",
    "    print (type(prediction))\n",
    "    prediction = prediction.as_data_frame().to_json()\n",
    "    print (type(prediction))\n",
    "    \n",
    "    return flask.jsonify({'success':True, \"predictions\": prediction})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_simple('192.168.1.62', 8081, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels =df[LABELS].apply(pd.Series.nunique)\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies =pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
    "                                                               label_dummies,\n",
    "                                                               size=0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")  \n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")  \n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")  \n",
    "print(y_test.info()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Load the holdout data: holdout\n",
    "holdout = pd.read_csv('HoldoutData.csv', index_col=0)\n",
    "\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "\n",
    "# Format predictions in DataFrame: prediction_df\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "\n",
    "\n",
    "# Save prediction_df to csv\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "# Submit the predictions for scoring: score\n",
    "score = score_submission(pred_path='predictions.csv')\n",
    "\n",
    "# Print score\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Fill missing values in df.Position_Extra\n",
    "df.Position_Extra.fillna('', inplace=True)\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern = TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.Position_Extra)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('', inplace=True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the basic token pattern\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df)\n",
    "\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_alphanumeric\n",
    "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import other necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Split and select numeric data only, no nans \n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Imputer object\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create training and test sets using only numeric data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "\n",
    "# Insantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Split out only the text data\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('vec', CountVectorizer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test,y_test)\n",
    "print(\"\\nAccuracy on sample data - just text data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Obtain the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "\n",
    "# Obtain the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
    "\n",
    "# Fit and transform the text data: just_text_data\n",
    "just_text_data = get_text_data.fit_transform(sample_df)\n",
    "\n",
    "# Fit and transform the numeric data: just_numeric_data\n",
    "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
    "\n",
    "# Print head to check results\n",
    "print('Text Data')\n",
    "print(just_text_data.head())\n",
    "print('\\nNumeric Data')\n",
    "print(just_numeric_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Split using ALL data in sample_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
    "process_and_join_features = FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "# Instantiate nested pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', process_and_join_features),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Fit pl to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all data: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fxy(x, y):\n",
    "    return x if x == 'SideA' else y\n",
    "\n",
    "intra['newcolumn'] = intra[['SideADeaths','SideBDeaths']].astype(str).apply(lambda x: fxy(x['SideADeaths'], x['SideBDeaths']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
